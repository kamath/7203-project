<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Numerical Analysis Mini-Project</title>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600;700;900&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Lato&display=swap');
        .animation-math path {
            stroke-dasharray: 10000;
            stroke-dashoffset: 10000;
            animation: dash 2s linear forwards;
        }
        
        @keyframes dash {
            from {
                stroke-width: 15px;
                fill: transparent;
            }
            50% {
                fill: transparent;
            }
            75% {
                stroke-dashoffset: 0;
                stroke-width: 15px;
            }
            100% {
                stroke-width: 0;
            }
        }
        
        .math {
            height: 500px;
            overflow: hidden;
            width: 100%;
        }
        
        .vertical-align-center {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
            vertical-align: middle;
        }
        
        h1 {
            font-family: 'Playfair Display';
            font-weight: 600 !important;
        }
        
        * {
            font-family: 'Lato';
            line-height: 1.5 !important;
        }
    </style>
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.4/dist/Chart.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min.js"></script>
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>
</head>

<body>
    <script>
        MathJax = {
            svg: {
                fontCache: 'local'
            }
        };
    </script>
    <div style="position: fixed;height:100%;width: 100%;z-index:-1000">
        <div style="position: absolute;bottom:0;width: 100%;z-index: 5">
            <span style="display: inline;"></span><button class="btn black" onclick="prevSlide()"><i class="material-icons">navigate_before</i></button></span>
            <span style="display: inline;"><button class="btn black" onclick="nextSlide()"><i class="material-icons">navigate_next</i></button></span>
        </div>
    </div>

    <span style="display: inline;"></span><button class="btn black" onclick="prevSlide()"><i class="material-icons">navigate_before</i></button></span>
    <span style="display: inline;"><button class="btn black" onclick="nextSlide()"><i class="material-icons">navigate_next</i></button></span>
    <span style="display: inline;"><p id="slidenums"></p></span>
    <div class="container" style="margin-top: 50px; margin-bottom: 50px;">
        <div class="slide">
            <div class="desc">
                <h1><a class="black-text animation-math">\(\text{Spectral Clustering with Laplacian Eigenmaps}\)</a></h1>
                <h3>MATH 7203 Mini-Project</h3>
                <h4>Anirudh Kamath</h4>
            </div>
        </div>

        <div class="slide">
            <h1>Spectral clustering</h1>
            <h5>- Derives a graph from the given data and uses the eigenvectors of this graph's Laplacian matrix to partition the data.</h5>
            <h5>- Captures the "geometry" of the data and part of a broader subclass of manifold learning.
            </h5>
            <h5>- <b>Manifold Learning:</b> Linear algorithms don't work when a nonlinear transformation is required.</h5>
            <h5>- Other methods in this umbrella include t-SNE and Isomap</h5>
        </div>

        <div class="slide">
            <h1>Spectral clustering</h1>
            <h5>- Derives a graph from the given data and uses the eigenvectors of this graph's Laplacian matrix to partition the data.</h5>
            <h5>- Captures the "geometry" of the data and part of a broader subclass of manifold learning.
            </h5>
            <h5>- <b>Manifold Learning:</b> Linear algorithms don't work when a nonlinear transformation is required.</h5>
            <h5>- Other methods in this umbrella include t-SNE and Isomap</h5>
            <div class="row">
                <div class="col m6"><canvas style="width: 50%" id="concentricBad" onclick="concentricBad()"></canvas></div>
                <div class="col m6"><canvas style="width: 50%" id="concentricGood" onclick="concentricGood()"></canvas></div>
            </div>
        </div>

        <div class="slide">
            <h1>Step 1: Construct Weight Matrix from given data</h1>
            <h5>We take our data \(X\) as a set of \(n\) row vectors...</h5>
            <div class="math-animate" toDisplay="X = \begin{bmatrix} - & \vec{x}_1 & - \\ - & \vec{x}_2 & - \\ & \vdots \\ - & \vec{x}_n & - \end{bmatrix}" scale="20" style="position: relative; top: 20px"></div>
        </div>

        <div class="slide">
            <h1>Step 1: Construct Weight Matrix from given data</h1>
            <h5>We take our data \(X\) as a set of \(n\) row vectors...</h5>
            <div class="math-animate dont-animate" toDisplay="X = \begin{bmatrix} - & \vec{x}_1 & - \\ - & \vec{x}_2 & - \\ & \vdots \\ - & \vec{x}_n & - \end{bmatrix}" scale="20" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>... and form some sort of weight matrix</h5>
            <div class="math-animate" toDisplay="W = \left[\begin{array}{c|c c c c}
            & x_1 & x_2 & \dots & x_n \\ 
            \hline
            x_1 & w_{11} & w_{12} & \dots & w_{1n} \\
            x_2 & w_{21} & w_{22} & \dots & w_{2n} \\
            \vdots & \vdots & \vdots & \ddots  & \vdots \\
            x_n & w_{n1} & w_{n2} & \dots & w_{nn}
            \end{array}\right]" scale="15" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>Where high weight indicates high similarity, and low weight indicates low similarity between \(x_i\) and \(x_j\)</h5>
            <h5>Similar to a graph <b>Adjacency Matrix</b> where row/column indices correspond to nodes, and the values at those indices correspond to edges/edge weights</h5>
        </div>

        <div class="slide">
            <h1>Step 1b: Similarity Metrics</h1>
            <h3>Conditional Connectivity</h3>
            <div class="math-animate" toDisplay="w_{ij}= 
        \begin{cases}
            1,& \left\lVert x_i - x_j\right\rVert \leq \sigma\\
            0,              & \text{otherwise}
        \end{cases}" scale="35" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>If \(x_i\) and \(x_j\) are "close" to each other by some metric \(\sigma\), then we say they are connected in the graph. Otherwise, they share no connection</h5>
        </div>

        <div class="slide">
            <h1>Step 1b: Similarity Metrics</h1>
            <h3>Conditional Connectivity</h3>
            <div class="math-animate dont-animate" toDisplay="w_{ij}= 
        \begin{cases}
            1,& \left\lVert x_i - x_j\right\rVert \leq \sigma\\
            0,              & \text{otherwise}
        \end{cases}" scale="35" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>If \(x_i\) and \(x_j\) are "close" to each other by some metric \(\sigma\), then we say they are connected in the graph. Otherwise, they share no connection</h5>
            <h3>Fully-Connected Graph with Varying Edge Weights</h3>
            <div class="math-animate" toDisplay="w_{ij}= \exp(-\frac{\left\lVert x_i - x_j\right\rVert^2}{2\sigma^2})" scale="35" style="position: relative; top: 20px"></div>
            <br><br>
            <h4>Main Idea: if \(x_i \tilde{=} x_j, w_{ij} \rightarrow 1\); else \(w_{ij} \rightarrow 0\)</h4>
            <h5>\(\sigma\) is an arbitrary constant to help define what "closeness" means between \(x_i\) and \(x_j\)</h5>
        </div>
        <div class="slide">
            <h1>Step 2: Construct Normalized Laplacian</h1>
            <h4>First, we compute the diagonal degree matrix, \(D\), where every entry is the sum of the edge weights of that given node</h4>
            <div class="math-animate" toDisplay="W = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \rightarrow D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}" scale="20"></div>
        </div>
        <div class="slide">
            <h1>Step 2: Construct Normalized Laplacian</h1>
            <h4>First, we compute the diagonal degree matrix, \(D\), where every entry is the sum of the edge weights of that given node</h4>
            <div class="math-animate dont-animate" toDisplay="W = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \rightarrow D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}" scale="20"></div>
            <h4>Then we compute the graph <b>Laplacian</b>, \(L\)</h4>
            <div class="math-animate" toDisplay="L = D - W" scale="20"></div>
        </div>
        <div class="slide">
            <h1>Step 2: Construct Normalized Laplacian</h1>
            <h4>First, we compute the diagonal degree matrix, \(D\), where every entry is the sum of the edge weights of that given node</h4>
            <div class="math-animate dont-animate" toDisplay="W = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \rightarrow D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}" scale="20"></div>
            <h4>Then we compute the graph <b>Laplacian</b>, \(L\)</h4>
            <div class="math-animate dont-animate" toDisplay="L = D - W" scale="20"></div>
            <h4>Or the normalized graph Laplacian, \(\mathcal{L}\)</h4>
            <div class="math-animate" toDisplay="\mathcal{L} = I_n - D^{-1/2}WD^{-1/2} \\= D^{-1/2}LD^{-1/2}" scale="20"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <h5>i.e. each eigenvector itself \(\phi_i\) is sorted per the corresponding eigenvalue \(\lambda_i\)</h5>
            <div class="math-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <h5>i.e. each eigenvector itself \(\phi_i\) is sorted per the corresponding eigenvalue \(\lambda_i\)</h5>
            <div class="math-animate dont-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
            <h4>\(\phi_1, \phi_2, \dots, \phi_n\) then form an orthonormal basis in \(R^n\) with eigenvalues sorted from lowest to highest:</h4>
            <div class="math-animate" toDisplay="\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \dots \leq \lambda_n" scale="40"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <h5>i.e. each eigenvector itself \(\phi_i\) is sorted per the corresponding eigenvalue \(\lambda_i\)</h5>
            <div class="math-animate dont-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
            <h4>\(\phi_1, \phi_2, \dots, \phi_n\) then form an orthonormal basis in \(R^n\) with eigenvalues sorted from lowest to highest:</h4>
            <div class="math-animate dont-animate" toDisplay="\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \dots \leq \lambda_n" scale="40"></div>
            <h4>We then form a matrix \(\Phi\) with </h4>
            <div class="math-animate" toDisplay="\Phi = \begin{bmatrix} - & \vec{\phi}_1 & - \\ - & \vec{\phi}_2 & - \\ & \vdots \\ - & \vec{\phi}_n & - \end{bmatrix}"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <h5>i.e. each eigenvector itself \(\phi_i\) is sorted per the corresponding eigenvalue \(\lambda_i\)</h5>
            <div class="math-animate dont-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
            <h4>\(\phi_1, \phi_2, \dots, \phi_n\) then form an orthonormal basis in \(R^n\) with eigenvalues sorted from lowest to highest:</h4>
            <div class="math-animate dont-animate" toDisplay="\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \dots \leq \lambda_n" scale="40"></div>
            <h4>We then form a matrix \(\Phi\) with </h4>
            <div class="math-animate dont-animate" toDisplay="\Phi = \begin{bmatrix} - & \vec{\phi}_1 & - \\ - & \vec{\phi}_2 & - \\ & \vdots \\ - & \vec{\phi}_n & - \end{bmatrix}"></div>
            <h3>This is called the <a class="black-text animation-math">\(\text{Laplacian Eigenmap}\)</a></h3>
        </div>
        <div class="slide">
            <h1>Step 4: K-Means Clustering on Eigenvector Matrix</h1>
            <div class="math-animate" toDisplay="\Phi = \begin{bmatrix} - & \vec{\phi}_1[:m] & - \\ - & \vec{\phi}_2[:m] & - \\ & \vdots \\ - & \vec{\phi}_n[:m] & - \end{bmatrix}"></div>
            <h5>From here, we can reduce our eigenvector matrix to have just \(m\) elements in each row such that \(1 \leq m \leq n \), and then perform K-Means clustering on this matrix.</h5>
        </div>
        <div class="slide">
            <h1>Example - Moons</h1>
            <div class="row">
                <div class="col m6"><canvas style="width: 50%" id="moonsBad" onclick="moonsBad()"></canvas></div>
                <div class="col m6"><canvas style="width: 50%" id="moonsGood" onclick="moonsGood()"></canvas></div>
            </div>
        </div>
        <div class="slide">
            <h1>Example - NBA Matchup Clustering</h1>
            <div class="row">
                <div class="col m12">
                    <h5>- The NBA Stats API provides various advanced statistics using player tracking systems</h5>
                    <h5>- Matchup data allows us to say "Player \(P_i\) guarded Player \(P_j\) for \(m\) minutes"</h5>
                    <h5>- Sounds like a graph with \(m\) as an edge weight and players as nodes!</h5>
                    <h5>- Caveat - this graph is directed, and therefore the adjacency matrix is <u>asymmetric</u>.</h5>
                    <h5>- I made the graph undirected by combining offensive/defensive matchup time, i.e. if \(P_x\) guarded \(P_y\) for \(m_{xy}\), and \(P_y\) guarded \(P_x\) for \(m_{yx}\) minutes, then \(W_{ij} = m_{xy} + m_{yx}\)</h5>
                </div>
            </div>
        </div>
        <div class="slide">
            <h1>Example - NBA Matchup Clustering</h1>
            <div class="row">
                <div class="col m6"><canvas style="width: 50%; z-index: 1005" id="NBA" onclick="NBA()"></canvas></div>
                <div class="col m6">
                    <h5>- The NBA Stats API provides various advanced statistics using player tracking systems</h5>
                    <h5>- Matchup data allows us to say "Player \(P_i\) guarded Player \(P_j\) for \(m\) minutes"</h5>
                    <h5>- Sounds like a graph with \(m\) as an edge weight and players as nodes!</h5>
                    <h5>- Caveat - this graph is directed, and therefore the adjacency matrix is <u>asymmetric</u>.</h5>
                    <h5>- I made the graph undirected by combining offensive/defensive matchup time, i.e. if \(P_x\) guarded \(P_y\) for \(m_{xy}\), and \(P_y\) guarded \(P_x\) for \(m_{yx}\) minutes, then \(W_{ij} = m_{xy} + m_{yx}\)</h5>
                </div>
            </div>
            <h4>Spectral clustering found a clear breakdown between small players (point/shooting guards), medium players (wings), and big players (power forwards/centers)</h4>
        </div>
        <div class="slide">
            <h1>Full Code</h1>
            <h5>GitHub - <a href="https://github.com/kamath/7203-project/blob/main/Laplacian%20Eigenmaps%20Spectral%20Clustering.ipynb">kamath/7203-project</a></h5>
            <h1>Sources</h1>
            <h5>- Shuyang Ling's Lecture Notes from <a href="https://cims.nyu.edu/~sling/MATH-SHU-236-2020-SPRING/MATH-SHU-236-Lecture-7-8-spectral.pdf">NYU Courant</a></h5>
            <h5>- Cory Malkin on <a href="https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045">Towards Data Science</a></h5>
        </div>
    </div>
    <script src="graphing.js"></script>
    <script src="mathDisplay.js"></script>
</body>

</html>