<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width">
    <title>Numerical Analysis Mini-Project</title>
    <style>
        .animation-math path {
            stroke-dasharray: 10000;
            stroke-dashoffset: 10000;
            animation: dash 2s linear forwards;
        }
        
        @keyframes dash {
            from {
                stroke-width: 15px;
                fill: transparent;
            }
            50% {
                fill: transparent;
            }
            75% {
                stroke-dashoffset: 0;
                stroke-width: 15px;
            }
            100% {
                stroke-width: 0;
            }
        }
        
        .math {
            height: 500px;
            overflow: hidden;
            width: 100%;
        }
        
        .vertical-align-center {
            position: relative;
            top: 50%;
            transform: translateY(-50%);
            vertical-align: middle;
        }
    </style>
    <!-- Compiled and minified CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

    <!-- Compiled and minified JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.4/dist/Chart.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min.js"></script>
</head>

<body>
    <script>
        MathJax = {
            svg: {
                fontCache: 'local'
            }
        };
    </script>
    <span style="display: inline;"><button onclick="prevSlide()">Prev</button></span>
    <span style="display: inline;"><button onclick="nextSlide()">Next</button></span>
    <span style="display: inline;"><p id="slidenums"></p></span>
    <div class="container" style="margin-top: 50px">
        <div class="slide">
            <div class="desc">
                <h1>Spectral Clustering with Laplacian Eigenmaps</h1>
                <h3>MATH 7203 Mini-Project</h3>
                <h4>Anirudh Kamath</h4>
                <canvas id="scatterLine" onclick="scatterLine()"></canvas>
            </div>
        </div>

        <div class="slide">
            <h1>Limitations of K-Nearest Neighbors</h1>
            <h5>Linear algorithms don't work when a nonlinear transformation is required. Case in point - concentric circles</h5>
            <div class="row">
                <div class="col m6"><canvas style="width: 50%" id="concentricBad" onclick="concentricBad()"></canvas></div>
                <div class="col m6"><canvas style="width: 50%" id="concentricGood" onclick="concentricGood()"></canvas></div>
            </div>
            <h3>Spectral clustering</h3>
            <h5>- Derives a graph from the given data and uses the eigenvectors of this graph's Laplacian matrix to partition the data.</h5>
            <h5>- Captures the "geometry" of the data and part of a broader subclass of
                <b>manifold learning</b>.
            </h5>
            <h5>- Other methods in this umbrella include t-SNE and Isomap</h5>
        </div>

        <div class="slide">
            <h1>Step 1: Construct Weight Matrix from given data</h1>
            <h5>We take our data \(X\) as a set of \(n\) row vectors...</h5>
            <div class="math-animate" toDisplay="X = \begin{bmatrix} - & \vec{x}_1 & - \\ - & \vec{x}_2 & - \\ & \vdots \\ - & \vec{x}_n & - \end{bmatrix}" scale="20" style="position: relative; top: 20px"></div>
        </div>

        <div class="slide">
            <h1>Step 1: Construct Weight Matrix from given data</h1>
            <h5>We take our data \(X\) as a set of \(n\) row vectors...</h5>
            <div class="math-animate dont-animate" toDisplay="X = \begin{bmatrix} - & \vec{x}_1 & - \\ - & \vec{x}_2 & - \\ & \vdots \\ - & \vec{x}_n & - \end{bmatrix}" scale="20" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>... and form some sort of weight matrix</h5>
            <div class="math-animate" toDisplay="W = \left[\begin{array}{c|c c c c}
            & x_1 & x_2 & \dots & x_n \\ 
            \hline
            x_1 & w_{11} & w_{12} & \dots & w_{1n} \\
            x_2 & w_{21} & w_{22} & \dots & w_{2n} \\
            \vdots & \vdots & \vdots & \ddots  & \vdots \\
            x_n & w_{n1} & w_{n2} & \dots & w_{nn}
            \end{array}\right]" scale="15" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>Where high weight indicates high similarity, and low weight indicates low similarity between \(x_i\) and \(x_j\)</h5>
            <h5>Similar to a graph <b>Adjacency Matrix</b> where row/column indices correspond to nodes, and the values at those indices correspond to edges/edge weights</h5>
        </div>

        <div class="slide">
            <h1>Step 1b: Similarity Metrics</h1>
            <h3>Conditional Connectivity</h3>
            <div class="math-animate" toDisplay="w_{ij}= 
        \begin{cases}
            1,& \left\lVert x_i - x_j\right\rVert \leq \sigma\\
            0,              & \text{otherwise}
        \end{cases}" scale="35" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>If \(x_i\) and \(x_j\) are "close" to each other by some metric \(\sigma\), then we say they are connected in the graph. Otherwise, they share no connection</h5>
        </div>

        <div class="slide">
            <h1>Step 1b: Similarity Metrics</h1>
            <h3>Conditional Connectivity</h3>
            <div class="math-animate dont-animate" toDisplay="w_{ij}= 
        \begin{cases}
            1,& \left\lVert x_i - x_j\right\rVert \leq \sigma\\
            0,              & \text{otherwise}
        \end{cases}" scale="35" style="position: relative; top: 20px"></div>
            <br><br>
            <h5>If \(x_i\) and \(x_j\) are "close" to each other by some metric \(\sigma\), then we say they are connected in the graph. Otherwise, they share no connection</h5>
            <h3>Fully-Connected Graph with Varying Edge Weights</h3>
            <div class="math-animate" toDisplay="w_{ij}= \exp(-\frac{\left\lVert x_i - x_j\right\rVert^2}{2\sigma^2})" scale="35" style="position: relative; top: 20px"></div>
            <br><br>
            <h4>Main Idea: if \(x_i \tilde{=} x_j, w_{ij} \rightarrow 1\); else \(w_{ij} \rightarrow 0\)</h4>
            <h5>\(\sigma\) is an arbitrary constant to help define what "closeness" means between \(x_i\) and \(x_j\)</h5>
        </div>
        <div class="slide">
            <h1>Step 2: Construct Normalized Laplacian</h1>
            <h4>First, we compute the diagonal degree matrix, \(D\), where every entry is the sum of the edge weights of that given node</h4>
            <div class="math-animate" toDisplay="W = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \rightarrow D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}" scale="20"></div>
        </div>
        <div class="slide">
            <h1>Step 2: Construct Normalized Laplacian</h1>
            <h4>First, we compute the diagonal degree matrix, \(D\), where every entry is the sum of the edge weights of that given node</h4>
            <div class="math-animate dont-animate" toDisplay="W = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \rightarrow D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}" scale="20"></div>
            <h4>Then we compute the graph <b>Laplacian</b>, \(L\)</h4>
            <div class="math-animate" toDisplay="L = D - W" scale="20"></div>
        </div>
        <div class="slide">
            <h1>Step 2: Construct Normalized Laplacian</h1>
            <h4>First, we compute the diagonal degree matrix, \(D\), where every entry is the sum of the edge weights of that given node</h4>
            <div class="math-animate dont-animate" toDisplay="W = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix} \rightarrow D = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1 \end{bmatrix}" scale="20"></div>
            <h4>Then we compute the graph <b>Laplacian</b>, \(L\)</h4>
            <div class="math-animate dont-animate" toDisplay="L = D - W" scale="20"></div>
            <h4>Or the normalized graph Laplacian, \(\mathcal{L}\)</h4>
            <div class="math-animate" toDisplay="\mathcal{L} = I_n - D^{-1/2}WD^{-1/2} \\= D^{-1/2}LD^{-1/2}" scale="20"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <div class="math-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <div class="math-animate dont-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
            <h4>\(\phi_1, \phi_2, \dots, \phi_n\) then form an orthonormal basis in \(R^n\) with eigenvalues sorted from lowest to highest:</h4>
            <div class="math-animate" toDisplay="\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \dots \leq \lambda_n" scale="40"></div>
        </div>
        <div class="slide">
            <h1>Step 3: Compute Laplacian Eigenvectors</h1>
            <h4>\(\phi_1, \phi_2, \dots, \phi_k\) corresponding to the \(k\) smallest eigenvalues of \(L\) or \(\mathcal{L}\)</h4>
            <div class="math-animate dont-animate" toDisplay="\mathcal{L}\phi_l = \lambda_l\phi_l,\;1 \leq l \leq n" scale="40"></div>
            <h4>\(\phi_1, \phi_2, \dots, \phi_n\) then form an orthonormal basis in \(R^n\) with eigenvalues sorted from lowest to highest:</h4>
            <div class="math-animate dont-animate" toDisplay="\lambda_1 \leq \lambda_2 \leq \lambda_3 \leq \dots \leq \lambda_n" scale="40"></div>
            <h4>We then form a matrix \(\Phi\) with </h4>
            <div class="math-animate" toDisplay="\Phi = \begin{bmatrix} - & \vec{\phi}_1 & - \\ - & \vec{\phi}_2 & - \\ & \vdots \\ - & \vec{\phi}_n & - \end{bmatrix}"></div>
            <h4><b>Theorem</b>: If the graph \(W\) has \(K\) connected components, then \(\mathcal{L}\) has \(K\) eigenvectors with an eigenvalue of 0.</h4>
        </div>
    </div>

    <script src="graphing.js"></script>
    <script src="mathDisplay.js"></script>
</body>

</html>